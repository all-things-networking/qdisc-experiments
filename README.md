# qdisc-experiments

<h2> To run the experiment
This experiment uses HHF Qdiscs.
To change hhf qdisc configuration: qdisc-config.json 

To run the experiment: sudo python3 mn-main.py


<h2> Outputs
After each run, the program generates a pair of graphs. 
The format of the files is : [number]_[s/r]_plot.jpg
[number] - The number is the number of heavy hitters for that specific run. Note that for small hh_flows_limit, this number could be a float.
[s/r] - whether it's the plot at the sender's (s) or receiver's (r) end.


<h2> How the experiment works
The host setup is linear, consisting of a sending host sends traffic to the receiving host. We modified the QDisc at the sender's end. The HHF qdisc is reset after each run. 

Suppose we abbreviate the parameters in the following way:
•	hhf_reset_timeout: C
•	hhf_admit_bytes: H
•	hhf_evict_timeout: E
•	hhf_non_hh_weight: W
•	hh_flows_limit: T
Let's have a total number N = 3 x T flows in our experiment. Also note that flows sending at a rate less than C x H will not be classified as heavy hitters based on the qdisc description. We want to do a series of runs.

run_0: Have T/2 flows send at 1.5 x C x H, the rest of the flows send at 0.5 x C x H.
run_i: Have (T/2 + i x T/4) send at 1.5 x C x H, the rest of the flows send at 0.5 x C x H.

Continue the runs until 2xT flows run at 1.5 x H / C, the rest of the flows send at 0.5 x H / C. The throughput of each flow is measured at both the secnder and receiver. 

The traffic for flows f_1, ..., f_N with rates r_1, ..., r_N are generated by the method below:
1.	For each flow, keep track of t_i, the last time they sent some traffic out, and b_i, the total number of bytes f_i has sent out so far. In the beginning, all b_i s will be zero and all t_i s will be the same and equal to the starting time of the run.
2.	Start from f_1. Using r_1, b_1, and (now() - t_1), calculate a_1 as the number of bytes f_1 is allowed to send. If it is not the very first time f_1 is sending some packets, send out a_1 bytes (you may need to spread them into multiple packets if a_1 is too large). If it is the very first time f_1 is sending some packets, send out max(a_1, H/N) bytes. Update t_1 and b_1 accordingly.
3.	Continue to f_2, f_3, .., f_N. Once all the flows are tried, restart sending from f_1.

Default QDisc parameters can be used, or something that would fit the experiment better. For example, it may be best to adjust T to a smaller value for the first set of experiments we run to get a better understanding of whether the experiment is appropriate or not.


<h2> Changing HHF QDisc Configuration
A brief explanation of HHF QDisc's parameters
hhf_reset_timeout
    Period to reset counter values in the multi_stage filter (default 40ms)
Need refreshing counter size because the counter values are increased by packet size.

hhf_admit_bytes
    Threshold to classify heavy-hitters (default 128KB)
Combined with hhf_reset_timeout, capture HHs sending > 128KB/40ms=25Mbps

hhf_evict_timeout
    Threshold to evict idle heavy-hitters (default 1s)
Should be large enough to avoid reordering during HH eviction.

hhf_non_hh_weight
    Weighted Deficit Round Robin (WDRR) weight for non-heavy-hitters (default 2)
i.e. Non HH : HH = 2:1

hh_flows_limit
Max number of heavy-hitter flow entries (default 2048)


<h2> Some other notes about HHF QDisc
Enqueuing: Classify, then put it into one of the correspoinging buckets.

Dequeuing: WDRR
There are two buckets for Weighted DRR - one for heavy-hitters and one for non-heavy-hitters.

hhf_classify(skb, sch):
The hash is probably the hashed flow-id of the skb. Computed as skb_get_hash_perturb(skb, &q->perturbation)
    
(Copied from the official documentation)
 Given a packet p:
- If the flow-id of p (e.g., TCP 5-tuple) is already in the exact-matching heavy-hitter flow table, denoted table T, then send p to the heavy-hitter bucket.
- Otherwise, forward p to the multi-stage filter, denoted filter F. 
    - If F decides that p belongs to a non-heavy-hitter flow, then send p to the non-heavy-hitter bucket.
    - Otherwise, if F decides that p belongs to a new heavy-hitter flow, then set up a new flow entry for the flow-id of p in the table T and send p to the heavy-hitter bucket.
    
For the multi-stage filter: k independent hash functions and k counter arrays. Packets are indexed into k counter arrays by k hash functions. Counters are then increased by packet sizes. 
If all k counters are large, then heavy-hitters; otherwise non-heavy-hitters. 
Theoretically no false negatives, but small chances of false positive (i.e. small packets classified as heavy-hitters, but they have some optimization mechanisms to reduce the probability)
