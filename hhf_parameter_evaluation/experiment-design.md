<h3> How the experiment works </h3>
The host setup is linear, consisting of a sending host sends traffic to the receiving host. We modified the QDisc at the sender's end. The HHF qdisc is reset after each run. 

Suppose we abbreviate the parameters in the following way:
•	hhf_reset_timeout: C
•	hhf_admit_bytes: H
•	hhf_evict_timeout: E
•	hhf_non_hh_weight: W
•	hh_flows_limit: T
Let's have a total number N = 3 x T flows in our experiment. Also note that flows sending at a rate less than C x H will not be classified as heavy hitters based on the qdisc description. We want to do a series of runs.

run_0: Have T/2 flows send at 1.5 x C x H, the rest of the flows send at 0.5 x C x H.
run_i: Have (T/2 + i x T/4) send at 1.5 x C x H, the rest of the flows send at 0.5 x C x H.

Continue the runs until 2xT flows run at 1.5 x H / C, the rest of the flows send at 0.5 x H / C. The throughput of each flow is measured at both the secnder and receiver. 

The traffic for flows f_1, ..., f_N with rates r_1, ..., r_N are generated by the method below:
1.	For each flow, keep track of t_i, the last time they sent some traffic out, and b_i, the total number of bytes f_i has sent out so far. In the beginning, all b_i s will be zero and all t_i s will be the same and equal to the starting time of the run.
2.	Start from f_1. Using r_1, b_1, and (now() - t_1), calculate a_1 as the number of bytes f_1 is allowed to send. If it is not the very first time f_1 is sending some packets, send out a_1 bytes (you may need to spread them into multiple packets if a_1 is too large). If it is the very first time f_1 is sending some packets, send out max(a_1, H/N) bytes. Update t_1 and b_1 accordingly.
3.	Continue to f_2, f_3, .., f_N. Once all the flows are tried, restart sending from f_1.

Default QDisc parameters can be used, or something that would fit the experiment better. For example, it may be best to adjust T to a smaller value for the first set of experiments we run to get a better understanding of whether the experiment is appropriate or not.

<h3> Some other notes about HHF QDisc </h3>
Enqueuing: Classify, then put it into one of the correspoinging buckets.

Dequeuing: WDRR
There are two buckets for Weighted DRR - one for heavy-hitters and one for non-heavy-hitters.

hhf_classify(skb, sch):
The hash is probably the hashed flow-id of the skb. Computed as skb_get_hash_perturb(skb, &q->perturbation)
    
(Copied from the official documentation)
 Given a packet p:
- If the flow-id of p (e.g., TCP 5-tuple) is already in the exact-matching heavy-hitter flow table, denoted table T, then send p to the heavy-hitter bucket.
- Otherwise, forward p to the multi-stage filter, denoted filter F. 
    - If F decides that p belongs to a non-heavy-hitter flow, then send p to the non-heavy-hitter bucket.
    - Otherwise, if F decides that p belongs to a new heavy-hitter flow, then set up a new flow entry for the flow-id of p in the table T and send p to the heavy-hitter bucket.
    
For the multi-stage filter: k independent hash functions and k counter arrays. Packets are indexed into k counter arrays by k hash functions. Counters are then increased by packet sizes. 
If all k counters are large, then heavy-hitters; otherwise non-heavy-hitters. 
Theoretically no false negatives, but small chances of false positive (i.e. small packets classified as heavy-hitters, but they have some optimization mechanisms to reduce the probability)